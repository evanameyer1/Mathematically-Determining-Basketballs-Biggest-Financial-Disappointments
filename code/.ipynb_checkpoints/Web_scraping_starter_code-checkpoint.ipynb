{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OJyR0G2F8lKj"
   },
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Web scraping with BeautifulSoup\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fVR_TWA_8lKm"
   },
   "source": [
    "\n",
    "### Learning Objectives\n",
    "\n",
    "**After this lesson, you will be able to:**\n",
    "\n",
    "- Identify whether it's ethical to scrape a website\n",
    "- Write selectors to pick out elements from HTML\n",
    "- Perform web scraping tasks using `requests` and `beautifulsoup`\n",
    "- Convert the results of a scraping task to a `pandas DataFrame` \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sjc_KChL8lKm"
   },
   "source": [
    "#  What is web scraping?\n",
    "---\n",
    "\n",
    "Sometimes the data we're after won't be available via a spreadsheet download, or an API. It might be embedded in a web page, or spread across several web pages. In this case, web scraping can be a good solution. \n",
    "\n",
    "Web scraping is a slightly more complicated data acquisition method. It involves two steps:\n",
    "\n",
    "* Grabbing (or 'scraping') the HTML underlying a website\n",
    "\n",
    "* Searching (or 'parsing') it to extract the information you're interested in. HTML is a language where different pieces of content on a website are sandwiched or enclosed inside 'tags' that describe exactly what that piece of content is. So, a large heading would be enclosed between opening and closing heading tags: ``<h1> My Heading <\\h1>``. By searching for particular tags in our scraped HTML, we can pick out and store the exact pieces of content we're interested in.\n",
    "\n",
    "Scraping is the programmatic equivalent of browsing a website, and copy-pasting content from the website into your own local file or spreadsheet. There are some basic rules to follow when scraping websites, to avoid getting into trouble:\n",
    "\n",
    "* **Don't scrape websites that ask you not to scrape them** It's important to avoid scraping websites that explicitly prohibit scrapers/crawlers/spiders/robots (these can sometimes be used to all mean scraping) in their Terms of Use or Terms and Conditions. Under special circumstances, it might be possible to get permission from a website to scrape them if you make direct contact with the owners, explain why you'd like to scrape their site, and what you'll do with the results. \n",
    "\n",
    "* **Ask permission** If possible, it's polite/good practise to drop the organisation behind a website a note or email to let them know you'll be scraping their site. \n",
    "\n",
    "* **Avoid scraping personal data**\n",
    "\n",
    "* **Be considerate** Don't send one million requests to a website in the space of one second! If you're looping through several URLs, add in a pause of a second or two using a function like ``time.sleep(5)`` to make sure you don't overwhelm a website's servers.\n",
    "\n",
    "The Office for National Statistics has published a good set of ethical scraping guidelines here: https://www.ons.gov.uk/aboutus/transparencyandgovernance/datastrategy/datapolicies/webscrapingpolicy\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zDCb24P8lKn"
   },
   "source": [
    "#  Making `get` requests\n",
    "---\n",
    "\n",
    "We're going to make a `get` request to scrape the HTML that drives `example.com`.\n",
    "\n",
    "Let's start by defining the URL we want to scrape, and using ``requests.get()`` to grab the HTML behind the site. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LO_14DB18lKr"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ml0NjWfNHGp_"
   },
   "source": [
    "Make a `get` request to a website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "C8fa1142HGp_"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DseSexlJ8lKt"
   },
   "source": [
    "Let's check the status code of the response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xrWUCDok8lKt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZS5cijPF8lKw"
   },
   "source": [
    "All good! Now let's access the text of the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNNwt1ec8lKx",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SWmHKvF7HGqE"
   },
   "source": [
    "## <font color='green'> Exercise 1: Making `get` requests</font>\n",
    "--- \n",
    "\n",
    "Try making `get` requests to the following URLs and observing the HTML you get back. Double check your results by opening your web browsers, visiting the website and using `View Source`.\n",
    "\n",
    "* https://toscrape.com/\n",
    "* https://en.wikipedia.org/wiki/Sensitivity_and_specificity\n",
    "* https://www.bbc.co.uk/news \n",
    "* https://www.reddit.com/\n",
    "* https://twitter.com/home\n",
    "\n",
    "Do your results look as expected? If not, why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2o9P3IWyHGqF"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "el3LN8r8HGqG"
   },
   "source": [
    "#  Writing HTML selectors\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t0tGD9KY8lK1"
   },
   "source": [
    "The HTML we scraped from `example.com` is currently one big, messy string that contains all the HTML from the `example.com` front page. How can we turn this into a searchable object? \n",
    "\n",
    "We use a library called ``beautiful soup`` to transform this string into a searchable object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bfYQWBk48lK1"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vMPff8_J8lK3"
   },
   "source": [
    "Let's now convert our raw text from the `toscrape.com` front page into a more easily searchable object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3ZZJqB0d8lK3",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you're curious about this html.parser, check out [the documentation](https://docs.python.org/3/library/html.parser.html)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VinWfVox8lK5"
   },
   "source": [
    "Our output doesn't look very different to ``example_text``, but let's check the types of our two variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rMMgzpMD8lK8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WFmr1iTO8lK6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Lb1qyw2m8lK9"
   },
   "source": [
    "Whereas ``example_text`` is a string, ``example_soup`` is a 'beautiful soup object.' \n",
    "\n",
    "This means we can very easily and precisely search for tagged HTML content using our new-found knowledge of how to write **selectors**.\n",
    "\n",
    "We know that the HTML tag for a hyperlink is ``'a'``. \n",
    "\n",
    "We can use this knowledge, together with the ``select`` method in beautiful soup, to extract every URL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gwzSZsAw8lK-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3x1SM-0n8lLA"
   },
   "source": [
    "This is a list; if there was more than one `a` element on the page, the list would contain all of these elements, because **select** finds **all** HTML elements that match our selector.\n",
    "\n",
    "We can access the first element of the list, and get the hyperlink text using the `.get_text()` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J1NvhYdu8lLA"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bMTNJyJU8lLC"
   },
   "source": [
    "We can also access the actual URL of the link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n57sS8OX8lLC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xHw9ot_T8lLE"
   },
   "source": [
    "## <font color='green'> Exercise 2: Writing selectors\n",
    "---\n",
    "\n",
    "* Write a selector to extract the title of the page, 'Scraping Sandbox'\n",
    "\n",
    "* Write a selector to extract the text in the two sub-headers (hint: check the HTML to see what elements they are!)\n",
    "\n",
    "* Write a selector to find out how many `<div>` elements on the page have a **class** of `col-md-6`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oejHAXCfHGqJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xcFbHeRKHGqJ"
   },
   "source": [
    "## <font color='green'> Exercise 3: Putting it all together</font>\n",
    "---\n",
    "\n",
    "Now let's put our web scraping skills to the test and retrieve book prices from http://books.toscrape.com/\n",
    "\n",
    "#### First, identify the HTML tag that contains the price of a book"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lgjQgmiqHGqJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j2oH7hr4HGqJ"
   },
   "source": [
    "#### Now write a CSS selector to retrieve all price elements from the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K7z6-EfnHGqJ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lVGQTkH-HGqK"
   },
   "source": [
    "#### Use `requests` to retrieve the HTML of the webpage, and `BeautifulSoup` and your selector to select all HTML tags from the page that contain a book's price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rBLGtm2BHGqK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WwujWArpHGqK"
   },
   "source": [
    "#### Use Python to extract the text from these HTML tags, so you're left with a list of strings representing prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQVLbil3HGqK"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CZm1yeKhHGqK"
   },
   "source": [
    "#### Clean up the strings (removing any unnecessary characters) and convert them into a `float` type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Okh7iAqvHGqK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hbPZRWqcHGqK"
   },
   "source": [
    "#### Create a new `pandas` DataFrame from these prices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xkBXT5seHGqK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RKxAf9VpHGqK"
   },
   "source": [
    "#### Use your new DataFrame to calculate the average price of a book in the store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gjbLcq6WHGqL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4kekMF1qHGqL"
   },
   "source": [
    "## <font color=\"green\">Stretch</font>\n",
    "---\n",
    "\n",
    "If you scroll down to the bottom of the page you'll notice this is only one page of a possible 50! In order to get a true average price of books on this site, we'll need to scrape them all.\n",
    "\n",
    "#### Identify the url structure of further pages of books. How can you change a url to navigate to a specific one of the 50 pages?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hPjddMa8HGqL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HCw0vW3AHGqL"
   },
   "source": [
    "#### Write a `for` loop to go through the pages 1-50 and retrieve prices, just like you did from the first page\n",
    "\n",
    "Reuse your code as much as you can, adapting it to allow your price list to include all 50 pages worth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B5ZJUW4QHGqL"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GyZMclLJHGqL"
   },
   "source": [
    "#### Revise your average book price calculation\n",
    "\n",
    "Use all 50 pages' worth of data to calculate the real average book price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fuiXgo55HGqM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X_VLpv7g8lLt"
   },
   "source": [
    "#  Repetitive scraping with `for` loops\n",
    "---\n",
    "\n",
    "We're familiar with control structures like `if` statements and `for` loops, so we have all the tools we need to carry out more advanced scraping tasks. \n",
    "\n",
    "Imagine we want to scrape Wikipedia to find the latitude and longitude of several cities.\n",
    "\n",
    "**Let's start by trying to Wikipedia to find the latitude and longitude of a single city.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4eDAIP4n8lLt"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mZPtciKV8lLv"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Re4f3IIXHGqM"
   },
   "source": [
    "We now have the HTML from a single city's page. Let's see where the latitude/longitude live on such a page:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Budapest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nz455C7XHGqM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T9Tz1xlvHGqM"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlRASii1HGqN"
   },
   "source": [
    "Easy! Now we could put this into a function, loop through all the cities we want and call the function each time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EWqqzT10HGqN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PTOUiGKEHGqN"
   },
   "source": [
    "While we loop through our cities, we can then just store the lat/long values each time in lists"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "y35bgHsAHGqN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lHSdVyLHGqN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ttwU19z_HGqN"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "maHIGkNS8lL4"
   },
   "source": [
    "Now we can store the data in a new DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f9iWOGuQ8lL4"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sOG8NR7zHGqO"
   },
   "source": [
    "#  Extracting data from tables\n",
    "---\n",
    "\n",
    "Often data is stored in HTML `table` elements. We *could* loop through each row and cell and extract the data cell by cell, but that would take a long time. There is a better way!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L2mT1l2XHGqO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3QBicLiLHGqO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m1MGFiH0HGqO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7SeQL4-kHGqO"
   },
   "source": [
    "You could then use `BeautifulSoup` to extract individual `<td>` elements that contain the data we're after...\n",
    "\n",
    "**or** the much simpler `pandas` approach:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "elp_WScHHGqO"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PBm0waGOHGqP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4KCOL_sHGqP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7tUDGWgVHGqP"
   },
   "source": [
    "The table we're after is already a DataFrame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKemAJYSHGqP",
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0HA1t2c2HGqP"
   },
   "source": [
    "It's not perfect but it's a pretty great start!\n",
    "\n",
    "Now it's just some quick cleaning and analysis to find which country has won the most World Cups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DYhgjh9OHGqP"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r4Zd1bODHGqQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ktKz0cUA8lL5"
   },
   "source": [
    "\n",
    "\n",
    "## <font color='green'> Exercise 4: Putting it all together</font>\n",
    "\n",
    "---\n",
    "    \n",
    "We will now combine all these concepts to scrape some new data.\n",
    "    \n",
    "The [National UFO Reporting Center Online Database](http://www.nuforc.org) stores information about reported UFO sightings.\n",
    "    \n",
    "We want to answer the following questions:\n",
    "    \n",
    "- is there a seasonal pattern in UFO sightings?\n",
    "- which US state sees the most UFO sightings? Within that, which cities report the most sightings?\n",
    "- what shape of UFO is the most common for people to see?\n",
    "\n",
    "#### 1. Visit http://www.nuforc.org/webreports/ndxevent.html, click through some of the rows and take a look at the format the data will be in"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P8Vax24ZHGqQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvrV8csmHGqQ"
   },
   "source": [
    "#### 2. Use Python and `requests` to read in the page at https://www.nuforc.org/webreports/ndxe202203.html and display the HTML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xQwBFTy7HGqQ"
   },
   "outputs": [],
   "source": [
    "ufo_url = \"https://www.nuforc.org/webreports/ndxe202203.html\"\n",
    "\n",
    "ufo_req = # FILL THIS IN\n",
    "\n",
    "# display the returned HTML below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nz8A79cyHGqQ"
   },
   "source": [
    "#### 3. Use `BeautifulSoup` to find all the table elements. How many are there?\n",
    "\n",
    "If you inspect the page you will see that there is a closing `</BODY>` and `</HTML>` tag near the top where there shouldn't be. Sometimes websites have bad HTML and your browser does its best to display it. It's only when you try to scrape it that these things cause an issue!\n",
    "\n",
    "There is a bit of code below to help you clean up the page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sCGnc0tBHGqQ"
   },
   "outputs": [],
   "source": [
    "# replace the FIRST instance of those </BODY> and </HTML> only\n",
    "ufo_html_cleaned = ufo_req.text.replace(\"</BODY>\\r</HTML>\", \"\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9BiVafWIHGqQ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0_7wkPXtHGqQ"
   },
   "source": [
    "#### 4. Use `pandas` to read in the contents of the table as a DataFrame\n",
    "\n",
    "How well did pandas manage to import the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6SL4vT2PHGqR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jJJPMU5NHGqR"
   },
   "source": [
    "#### 5. Deduce the url pattern for the monthly sightings\n",
    "\n",
    "The sightings on the page http://www.nuforc.org/webreports/ndxevent.html are organised by month. Inspect the monthly urls to determine a pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "V6-xZCh5HGqR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qz9Guh2vHGqR"
   },
   "source": [
    "#### 6. Write a function that reads in a month of UFO sightings\n",
    "\n",
    "Your function should:\n",
    "\n",
    "- take as input a year and a month (in whatever format you see fit)\n",
    "- request the appropriate HTML\n",
    "- extract the `<table>` into a DataFrame. Combine the code from previous questions to achieve this. Don't forget to include the code to clean up the HTML with those problematic tags!\n",
    "\n",
    "Test it with March 2022 and make sure your DataFrame looks the same as above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "s5r6XMIWHGqR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aKFiqKEJHGqS"
   },
   "source": [
    "#### 7. Write a `for` loop to call your function for every month since 2019\n",
    "\n",
    "Store each returned DataFrame in the list below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-bYKHPO6HGqS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9JjQV1zHGqS"
   },
   "source": [
    "#### 8. Use the `pandas` `.concat()` method to combine your DataFrames into a single dataset of UFO sightings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nOpzzKh_HGqS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "weQQGD7ZHGqS"
   },
   "source": [
    "#### 9. Convert the `Date / Time` column to a `datetime`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UNce4MT6HGqS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BJzkjV5SHGqS"
   },
   "source": [
    "#### 10. Answer the \"research\" questions\n",
    "\n",
    "\n",
    "* What are the most common shapes of UFOs that people sight?\n",
    "* Is there a seasonal pattern in sightings?\n",
    "* How (if at all) has the pandemic changed UFO sighting behaviour?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MhKt7k6lHGqS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "6.13 Web scraping with BeautifulSoup.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
